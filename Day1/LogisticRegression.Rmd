---
title: "Exposure-response modeling for binary data"
output:
  slidy_presentation:
#    theme: cerulean
    fig_width: 9
    fig_height: 5
    font_adjustment: 3
    transition: none
    css: slidystyles.css
    footer: Metrum Research Group &copy 2021
    mathjax: local
    self_contained: false
    df_print: tibble
#bibliography: alpha.bib
#nocite: | 
#    @2795, @2192, @3684
---


```{r,echo=FALSE,message=FALSE}
knitr::opts_chunk$set(comment='.',fig.align=TRUE,message=FALSE,warning=FALSE)
library(tidyverse)
library(stringr)
library(haven)
library(survival)
library(GGally)
library(mgcv)
library(binom)
library(texreg)
library(DHARMa)
library(brms)
set.seed(314159)

theme_set(theme_bw())

expit <- function(x) 1 / (1+exp(-x))
``` 

# Outline

* Encountering binary data
* Groundwork
   * Notational conventions
   * Modeling from a probabalistic viewpoint
   
* Bernoulli distribution
   * Defining models
   * Maximum likelihood

* Visualizing relationships
* Models, model checking, and measures of effect
  
# Key learning objectives

At the end of today's session, I hope you will know:

* How to make effective exploratory analysis plots for binary data
* The basic concepts of maximum likelihood estimation for binary data models
* How to fit and evaluate binary data models
* How to interpret model terms


# What do we mean by binary data?

* Outcomes that have two possible values

* Can be categorical by nature or created by discretizing a categorical or continuous variable

* Examples
   * Objective response per RECIST (response / non-response)
   * Incident adverse event (yes / no)
   * Alive without disease progression vs. progressive disease or died
   * Coin flip (heads / tails)

# What makes binary data interesting?

-  If you are coming from a Pop PK background, moving away from models with Normal (or log-Normal) residuals may force you to think at a new level of abstraction. 

-  The Bernoulli distribution for binary data is (in most respects) as simple a statistical distribution as there is. Good place to start for many basic statistical concepts.  

- There are fewer choices to make and fewer assumptions to check when dealing with binary data (e.g. as compared to time-to-event data). 


# Basic Notation

For now, we will use this high-level notation: 

* Greek letters are <red>model parameters</red>
     * $\mu$  model parameter ("intercept")
     * $\beta$ model parameter (coefficient for effect of exposure or covariate)

* Upper case Roman letters are <red>random variables</red>
     * $C$, $D$, $T$ : exposure (think of a steady-state exposure metric for now, e.g. $\mathrm{CAVG}_\mathrm{ss}$), or dose, or just treatment indicator. 
     * $X$ : covariates
     * $Y$ : As-yet-unrealized / unobserved response ("DV")

* Lower case Roman letters are <red>observed values</red>
   * $y$ : Observed value for $Y$ 


# Probability versus statistics

In some cases (e.g., when setting up a model) we will be thinking  in the  data generating / probability direction: 

$$
\begin{eqnarray*} 
\mu,\, \beta,\, C,\, X & \stackrel{\mathrm{Probability}}{\longrightarrow} & Y 
\end{eqnarray*}
$$

In other cases we will be thinking in the model estimating / statistics direction:

$$
\begin{eqnarray*} 
\mu,\, \beta & \stackrel{\mathrm{Statistics}}{\longleftarrow} &  C,\, X,\, y 
\end{eqnarray*}
$$

<!-- # Modeling from a probabilistic point of view -->

<!-- * Those of you who were initially trained in a physical or biological science may have started with a deterministic view of modeling. -->

<!--   * Variability and uncertainty and the statistical tools to -->
<!--       deal with them were obligatory nuisances to deal with "noise" -->
<!--       but not the focus of the modeling. -->

<!--   *  This began to turn around for many of us with the increasing -->
<!--       use of mixed effects modeling in which probability distributions -->
<!--       are used to describe the "unexplained" portion of -->
<!--       inter-individual variability. -->

<!--   * Even then you may have continued to view residual -->
<!--       variability as more of a nuisance than as an integral component -->
<!--       of the model. -->

<!-- * What we will see in this course is that such notions of modeling do not -->
<!--     translate well to modeling of categorical, count or time-to-event -->
<!--     data. A probabilistic perspective is far more useful. -->


# Probability models: continuous variable

Throughout the course we will conceptualize models from a probabalistic or `data generating` viewpoint.

We'll use expressions like:

$$\text{Height} \sim \text{Normal}(\mu, \sigma)$$

:::{.notebox}
**Read as:**

> Height follows a normal distribution with mean $\mu$ and standard deviation $\sigma$

:::
If we know $\mu$ and $\sigma$ we can 

  * Make probabalistic statements about height in the population
  * Simulate heights
  
# Probability models: binary variable

Suppose we define a binary variable as

$$
Tall = \left\{ \begin{array}{ll}
               1 & Height > 200 cm \\
               0 & Height \leq 200 cm
               \end{array}
       \right.
$$


> NB: From here on, we'll assume binary random variables are defined as 0 / 1

Then our model might be

$$\text{Tall} \sim \text{Bernoulli}(\pi)$$

:::{.notebox}

**Read as:**

> Being tall follows a Bernoulli distribution with the probability of being tall equal to $\pi$

:::

If we know $\pi$, we can

  * Make probabalistic statements about the number of tall people in a random sample from the population
  * Simulate data


# Probability density (mass) function

Both of these link to specific probability density or probability mass functions:

The _normal_ (or _Gaussian_) pdf for height
$$
    p(height \,|\, \mu, \sigma ) = \frac{1}{\sqrt{2\pi}
      \sigma} e^{-\frac{1}{2\sigma^2} \left(height - \mu \right)^2}
$$

and the _Bernoulli_ pmf for Tall

$$
    p( tall \,|\, \pi ) = \pi^\text{tall} (1-\pi)^{1-\text{tall}}
$$


# The likelihood function

   
* Suppose you observed a measured value $Y = y_{obs}$.

    <!-- * That value is no longer a random variable since we know its -->
    <!-- value. -->

* If we view the pdf or pmf <grn>as a function of the parameters</grn>, conditional on some observed data $y_{obs}$, we refer to the function as a <red>likelihood function</red>
<!-- * If we insert that observed value into our probability -->
<!--     distribution function we now refer to that function as a -->
<!--     ***likelihood function***. -->

* It is the ***same function*** as the PDF or pmf, but we now view it as a
    function of the parameters given the data instead of as a function
    of the data given the parameters.
 
<p align="center">
<red>$L\left(\theta \,|\,y_{obs}, x\right)$</red> $=$ <grn>$p\left(y_{obs} \,|\,\theta,
  x\right)$</grn>
</p>

# Modeling from a probabilistic point of view: The likelihood function

$$ L\left(\theta \,|\,y_{obs}, x\right) = p\left(y_{obs} \,|\, \theta,
  x\right) $$
 
* During model development we generally do not know the values of
  the parameters $\theta$ and use the observed data to estimate those
  parameters.

* The likelihood function contains information about what those
  parameter values might be.

* We will talk about two different approaches that exploit the
  likelihood function to estimate $\theta$:
   
  * Maximum likelihood estimation
  * Bayesian statistical analysis
   
 




# The Bernoulli likelihood for one observation

<red>Likelihood</red> for a single Bernoulli observation

$$
l(\pi \, | \, Y_i = y_i) = P(Y_i=y_i) = \left\{ \begin{array}{ll} \pi & y_i = 1 \\ 1-\pi & y_i = 0 \end{array} \right.
$$
We often see this written more compactly as

$$
l(\pi \, | \, Y_i = y_i) = \pi^{y_i}(1-\pi)^{(1-y_i)} 
$$
<br>

# The Bernoulli joint likelihood function

<red>Joint likelihood</red> for a sample of independent Bernoulli observations

$$
\begin{eqnarray*}
l(\pi | \mathbf{Y}=\mathbf{y}) & = & \prod_{i=1}^{n} P(Y_i=y_i) \\
  & = &   \prod_{i=1}^{n}\pi^{y_i}(1-\pi)^{(1-y_i)} \\
& = & \pi^{(\# \, \text{of "ones"})}(1-\pi)^{n - (\# \, \text{of "ones"})}
\end{eqnarray*}
$$


<red>Joint log likelihood:</red>

$$
\begin{eqnarray*}
L(\pi \,\, | \,\, \mathbf{Y}=\mathbf{y}) & = & l(\pi| \mathbf{Y}=\mathbf{y}) \\
 & = & (\# \, \text{of "ones"}) \log(\pi)
 + (n-\# \, \text{of "ones"}) \log(1-\pi) 
\end{eqnarray*}
$$

# Maximum likelihood estimation for a simple model


<red>Maximum likelihood estimates</red> are the values of the parameters ($p$) which maximize the likelihood.

> Derivative of joint log likelihood: 

$$
\begin{align*}
\frac{\mathrm{d}L}{\mathrm{d}\pi} &= \frac{(\# \, \text{of successes})}{\pi} - \frac{(n- \# \, \text{of successes})}{1-\pi}
\end{align*}
$$


$$
\frac{\mathrm{d}L}{\mathrm{d}\pi} = 0 \iff  \pi = \frac{(\# \, \text{of successes})}{n}
$$

* Estimator for $\hat \pi$ is as expected from probability perspective
* Note: common value of $\pi$ for all subjects


# Maximum likelihood for regression models

* When we have a full logistic regression model with covariates, there is no analytical solution to the likelihood equations. 
* However there is a numerical root finder that is specially tailored to the structure of logistic (and all GLM) models: the `Newton-Raphson` method ([see @2795]).


# Workbook: Exploring the binomial density

* Biomial density
* Likelihood function


# Binary Data Example


* This data set is comprised of a two-week study (protocol A) and a six-week study (protocol B)

* The data set includes patients of type `PT2`, a patient type that (let's say) has not been studied at higher dose levels.

* Suppose further that a dose level under consideration for a phase 2 study in `PT2` would have typical value exposures near 2.5 ug/mL.

* The objective of *this* anlaysis will be to determine whether the AE rate will be "sufficiently low" in `PT2` at that exposure.

# Data snapshot

\small 

```{r, echo=FALSE}
load('../data/aedat.RDS')
source('preprocess_data.R')

aedat %>% 
  mutate(across(where(is.numeric), pmtables::sig)) %>% 
  select(STUDYID, USUBJID, PBO, CAVGSS, AE01, AETOXGR) %>% 
  head(n=5)
```

* `STUDYID`: protocol
* `USUBJID`: unique subject id
* `AE01`: adverse event of grade 3 or higher


# Visualizing relationships

Objective:

> Plot the probability of a grade 3 or higher AE vs predictor 

Types of plots will depend on the predictor variable:

* Categorical variable
* Continuous variable

# Relationship with categorical variable

* x-axis: Categorical variable (`STUDYID`)
* y-axis: Probability of Grade 3+ AE (`AE01`)
* geom: `bar`, `point`


# Probability vs categorical variable {.sourceCode.r.small}
::: columns

:::: column
```{r, 'cat-plot', fig.show='hide'}
aedat %>% 
  group_by(STUDYID) %>% 
  summarise(x = sum(AE01), n=n()) %>% 
  mutate(phat = x / n,
         lcl = binom.confint(x,n,methods = "wilson")$lower,
         ucl = binom.confint(x,n,methods = "wilson")$upper) %>% 
  ggplot(aes(x=STUDYID, y=phat)) + 
  geom_col() +
  geom_errorbar(aes(ymin=lcl, ymax=ucl), 
                width = 0.2) +
  labs(x='Study', y='Probability of severe AE')
```

Note: `binom.confint` is in the `binom` package

::::

:::: column
```{r, ref.label='cat-plot', echo=FALSE, out.width="95%"}

```
::::

:::


# Alternative to bar plot {.sourceCode.r.small}
::: columns

:::: column
```{r, 'cat-plot2', fig.show='hide'}
aedat %>% 
  group_by(STUDYID) %>% 
  summarise(x = sum(AE01), n=n()) %>% 
  mutate(phat = x / n,
         lcl = binom.confint(x,n,methods = "wilson")$lower,
         ucl = binom.confint(x,n,methods = "wilson")$upper) %>% 
  ggplot(aes(x=STUDYID, y=phat)) + 
  geom_point() +
  geom_errorbar(aes(ymin=lcl, ymax=ucl), 
                width = 0.2) +
  labs(x='Study', y='Probability of severe AE')
```

Note: `binom.confint` is in the `binom` package

::::

:::: column
```{r, ref.label='cat-plot2', echo=FALSE, out.width="95%"}

```
::::

:::



# Probability vs a continuous variable {.sourceCode.r.small}

:::{.columns}

::::{.column}
```{r, echo=FALSE}
dat_eda <- aedat %>% group_by(Quartile) %>% mutate(MedConc = median(CAVGSS))
```

```{r, 'cont-er', fig.show='hide'}
ggplot() +
  # Add tick marks at top and bottom
  geom_rug(data = filter(dat_eda, AE01 == 0),
           aes(x = CAVGSS), sides = "b") +
  geom_rug(data = filter(dat_eda, AE01 == 1),
           aes(x = CAVGSS), sides = "t") +
  # Add snooth mean function
  geom_smooth(data = dat_eda,
              aes(x = CAVGSS, y = AE01),
              method='gam', formula=y~s(x),
              method.args = list(family='binomial')) +
  # Add points and CIs
  stat_summary(
    data = dat_eda,
    aes(x = MedConc, y = AE01, group = MedConc),
    fun = function(y) sum(y) / length(y),
    geom = "point"
  ) +
  stat_summary(
    data = dat_eda,
    aes(x = MedConc, y = AE01),   
    # Wilson CI recommended by Agresti and Coull (2000) review paper
    fun.min = function(y) {
      binom.confint(sum(y),length(y),
                    methods = "wilson")$lower
      },
    fun.max = function(y) {
      binom.confint(sum(y), length(y),
                    methods = "wilson")$upper
      },
    geom = "errorbar"
  ) +
  labs(x='Steady-state Cavg', y='Probability of severe AE')
```
::::

::::{.column}
```{r, ref.label='cont-er', echo=FALSE}

```

::::

:::


# Plotting tips

* Break exposure into quartiles, compare incidence rate across quartiles
  * Textbook variance formula $\frac{p(1-p)}{n}$ isn't the best choice with small sample sizes and probabilities near zero or one
  * Recommendation: Use Wilson interval (implemented in  `binom.confint`)
* Rug plots (`geom_rug`)
* Stratify by other covariates of interest


# Workbook: Visualizing binary data





# Measures of effect: odds ratio

* `Conditional probability` of an event, conditional on treatment "1".

$$ P(Y=1 \, | \, T = 1) $$

* `Odds` of an event, conditional on treatment "1": 

$$ \frac{P(Y=1 \, | \, T = 1)}{P(Y=0 \, | \, T = 1)} = \frac{P(Y=1 \, | \, T = 1)}{1 - P(Y=1 \, | \, T = 1)}$$

* `Odds ratio` of an event, for treatment "1" versus treatment "0":

$$ \frac{\left.P(Y=1 \, | \, T = 1)\, \right/ P(Y=0 \, | \, T = 1)}{\left.P(Y=1 \, | \, T = 0) \, \right/ P(Y=0 \, | \, T = 0) }$$


# Measures of effect: relative risk

Relative risk of an event, for treatment "1" versus treatment "0":

$$ \frac{P(Y=1 \, | \, T = 1)}{P(Y=1 \, | \, T = 0)}$$
Anecdotally, this is often the preferred / most interpretable way to quantify efficacy.

NB: Odds ratio and relative risk are sometimes confused with each other. Note the difference. 


# Other measures of effect for binary data
    
Difference in probability of events, for treatment "1" versus treatment "0":
    

$$ P(Y=1 \, | \, T = 1) - P(Y=1 \, | \, T = 0) $$
Often undesirable: do you want to treat the difference between 3% and 5% the same way that you treat the difference between 23% and 25% ? 


# The logit transform

* The logit, or "log odds" function

$$\mathrm{logit}(p)=\log\left(\frac{p}{1-p}\right)$$

* The standard logistic function (also called the "expit") is the inverse of the logit:

$$\begin{align*}
p &= \mathrm{expit}(x) \\[1em]
  & = \frac{1}{1+\exp(-x)} \\[1em]
  & = \frac{\exp(x)}{1 + \exp(x)}
  \end{align*}$$ 


# Other "link functions"

* The `logit` function takes us from the unit interval to the full Real line: 

$$ (0, 1) \stackrel{\mathrm{expit}}{\longrightarrow} \mathbb{R} $$

* An alternative "link" function is the `probit` : 

$$ \mathrm{probit}(p) = \Phi^{-1}(p)$$ 
where $\Phi$ is the Normal Cumulative Density Function (CDF): 

$$ \Phi(x) = P(\mbox{Std. Normal Variate} < x)  $$

* An alternative "link" function is the `complementary log-log` : 
    * $\text{cloglog}(p) = \log( - \log(1 - p)$ 
    * Used much less frequently than logit or probit
    * Its an asymmetrical link function
    * Shows up when transforming other models to binary outcomes, e.g. Poisson model to binary model

# More About Link Functions

```{r, echo=FALSE, message=FALSE, out.width="80%", fig.align='center'}  
#, width=4*1.6, height=4}

data.frame(x = c(-5, 5)) %>%
  ggplot(aes(x = x), ) +
  stat_function(fun = ~1-exp(-exp(.x)), aes(colour = "Complementary log-log"), size = 2, alpha = 0.6, show.legend = TRUE) +
  stat_function(fun = pnorm, aes(colour = "Probit"), size = 2, alpha = 0.6, show.legend = TRUE) +
  stat_function(fun = expit, aes(colour = "Logit"), size = 2, alpha = 0.5, show.legend = TRUE) +
  scale_colour_manual(name = "Link Function", values = c("Logit" = "black", "Probit" = "orange", "Complementary log-log" = "blue")) +
  labs(x = "Linear Predictor", y = "Probability")
  

```


# Workbook: Exploring odds ratios, relative risks, and the logit transformation


# A logit-link GLM

A GLM with a logit link and Bernoulli (or more generally, Binomial) distribution is referred to as a <red>logistic regression</red>. 

A logistic regression with exposure ($C_i$) as the sole predictor would be expressed as: 

$$ Y_i \sim \mathrm{Ber}(\pi_i) \,\,\, \text{where} \,\,\, \pi_i = \mathrm{expit}(\mu + \beta C_i) \,\, ; \,\,i = 1,\ldots, n$$

Or equivalently:

$$ Y_i \sim \mathrm{Ber}(\pi_i) \,\,\, \text{where} \,\,\, \mathrm{logit}(\pi_i) = \mu + \beta C_i \,\, ; \,\,i = 1,\ldots, n$$


# Logistic regression is a type of GLM

* The `generalized` in ***generalized linear model*** refers to the non-Normal residuals (in this case, Bernoulli residuals). 

* The `linear` in ***generalized linear model*** refers to the fact that the right hand side of, e.g. 
    
$$ \mathrm{logit}(\pi_i) = \mu + \beta C_i $$
is <red>linear in the parameters</red> (i.e. it is a linear function of $\mu$ and $\beta$).


# Quiz 

Which (if any) of the following is linear in the parameters?

$$\mu + \beta \log(C)  \,\,\,\,\,\,\,\,\,\,\, \mu + \frac{\beta}{C} \,\,\,\,\,\,\,\,\,\,\, \mu + \frac{\beta_1 C}{(\beta_2 + C)}$$


# Anatomy of a GLM

Taking the following model as an example: 

$$ Y_i \sim \mathrm{Ber}(\pi_i) \,\,\, \text{where} \,\,\, \mathrm{logit}(\pi_i) = \mu + \beta C^*_i \,\, ; \,\,i = 1,\ldots, n$$

Standard terminology to refer to the model components is:

* <grn>$Y_i \sim \mathrm{Ber}(\pi_i)$</grn> is the <red>distribution</red> component of the model. (Sometimes also called the `random` component of the model, but we avoid that terminology is it becomes ambiguous in a `GLMM` context that includes random effects).
* The <grn>logit</grn> transformation is the <red>link</red> function. 
* <grn>$\mu + \beta C^*_i$</grn> is the <red>linear predictor</red>.




# Key Assumptions for Logistic Regression 

* Residual distribution assumptions: 
    * Observations are independent (conditional on covariates and exposure).
    * Observations associated with identical covariate values are identically distributed. 
    * For Bernoulli residuals, the only remaining distributional "assumption" is that the data are binary (pretty easy to check!). 
* Linear predictor:
    * All relevant predictors are in the model and suitably transformed. 
    * Interaction terms included where necessary. 
    * "Plays nicely" with link function. 
* Missing data assumptions. 

# Fitting a logistic regression in R

```
mod01_glm <- glm(AE01 ~ CAVGSS + BWT + PTTYPE + SEXTXT,
                 data = aedat,
                 family = binomial(link='logit'))
```

* <red>`AE01`</red>: binary outcome variable
* <red>`CAVGSS + BWT + PTTYPE + SEXTXT`</red>: linear predictor
* <red>`family = binomial(link='logit')`</red>: distribution and link function.  
    * `logit` is the default link
    * other link functions include `probit` and `cloglog`  

# General R model syntax

The right-hand side of the formula syntax: 

```{r, eval=FALSE}
~ CAVGSS + BWT + PTTYPE + SEXTXT
```

specifies that the linear predictor is 

$$ 
\begin{eqnarray*}
\beta_0 & + & \beta_\mathrm{C} \mathrm{C} + \beta_\mathrm{W} \mathrm{W} + \\
&& \beta_\mathrm{PT1} \mathrm{I}_{\mathrm{PT}=\mathrm{PT1}} + 
\beta_\mathrm{PT2} \mathrm{I}_{\mathrm{PT}=\mathrm{PT2}} + 
\beta_\mathrm{M} \mathrm{I}_{\mathrm{SEX}=\mathrm{M}} 
\end{eqnarray*}
$$ 

<!-- NB: formula syntax gets interpreted differently in R depending on the type of model being fit. E.g. models are specified differently in `nls`, `nlme`, `gnlm`, to name a few.  -->

# Factor variables in R models

* Most modeling and plotting functions in R treat factor variables differently from numerical variables. 
* For modeling functions, the first factor level is treated as the "reference level".
* Choice of reference level determines the interpretation of the model intercept. 
* As our field uses *another tool* that doesn't allow character variables, you will probably end up 
with some categorical variables that have numerical values...
    * Remember to "factorize" your categorical variables!
    * `as.factor(SEX)` can even be used directly in model formula!


# Output of fitted model

```{r, echo=FALSE}
mod01_glm <- glm(AE01 ~ CAVGSS + BWT + PTTYPE + SEXTXT,
                 data = aedat,
                 family = binomial(link='logit'))
```

```{r}
summary(mod01_glm)
```


# Interpreting coefficients in logit-link models: Intercept

* The logistic or expit function can be applied to the intercept or linear predictor to transform it to a probability. 
    * $\textit{expit}(\beta_0) = \frac{1}{1 + exp(-\beta_0)}$ 
    * The probability of an AE when all predictors are 0
    

# Interpretation of effects for categorical covariates

* Let $p_1$ refer to probability of AE for a `PT1` patient and 
* Let $p_0$ refer to probability of AE for `HV` with exactly the same exposure and covariate values. 
* Then:

$$ 
\mathrm{logit}(p_{1}) - \mathrm{logit}(p_{0}) = \beta_{\mathrm{PT1}}
$$ 
And since the logit function is the log-odds function, that implies: 

$$
\frac{p_{1}/(1-p_{1})}{p_{0}/(1-p_0)} = \exp(\beta_{\mathrm{PT1}})
$$
In other words, $\exp(\beta_{\mathrm{PT1}})$ is the odds ratio for the effect of being type `PT1` versus `HV`. 

# Interpretation of effects for continuous covariates

* For continuous covariates, the odds ratio adjustments are exponential "per unit"
    *  For example, the coefficient for body weight is $\beta_{WT}$ = `r pmtables::sig(mod01_glm$coefficients['BWT'])` 
    * So, a 10 kg difference in weight corresponds to an odds increase of exp(10 $\times$ `r pmtables::sig(mod01_glm$coefficients['BWT'])`  ) =  `r pmtables::sig(exp(10*mod01_glm$coefficients['BWT']))` 
* May be advantageous to scale exposure (e.g. divide by 1000) in order to avoid exponentiated coefficients like $1.000123$, which might indicate a consequential effect despite being very close to 1
    * try $1.000123^{1000}$.

# Interpreting covariate effects on the probability scale

* A unit of improvement in $x$ means different things depending upon the reference $x$
* Largest change always occurs at the inflection point
    * The derivative of the logistic function is maximized at $p=0.5$ and is maximized by $\beta/4$

```{r, echo=FALSE, message=FALSE, width=4*1.6, height=4}
data.frame(
  LP = seq(-6,6,.1)
) %>%
  mutate(prob=expit(LP)) %>%
  ggplot(aes(x=LP,y=prob)) + geom_line() + 
  theme_bw() + 
  geom_segment(aes(x=0,y=.5,xend=1,yend=.5),lty=2) +
  geom_segment(aes(x=1,y=.5,xend=1,yend=expit(1)),lty=2) +
  geom_segment(aes(x=3,y=expit(3),xend=4,yend=expit(3)),lty=2) +
  geom_segment(aes(x=4,y=expit(3),xend=4,yend=expit(4)),lty=2) +
  xlab('Linear predictor (i.e., "x")') + ylab("Probability") +
  geom_label(data=NULL, parse=TRUE,
             aes(x=-4, y=.75, 
                 # label=paste("mu==0~beta==1"))
                 label=paste("logit(p)==0 + 1*x")
             )) + 
  geom_label(data=NULL, aes(x=.5,y=.45,label="1 unit")) +
  geom_label(data=NULL, aes(x=3.5,y=.9,label="1 unit")) +
  geom_label(data=NULL, aes(x=1.75,y=.6,label=paste("beta/4")), parse=TRUE)

```


# Interpretation of covariate effects for probit-link models

* You really can't.
* That's part of the reason for the popularity of logit-link models.
* How much does that really matter?
    * It's certainly nice to have directly interpretable coefficients.
    * But in many cases, the predictive inferences matter more than the direct inferences on parameters.
    * Recommendation: choose link function based on what fits the data better, not based on mathematical convenience. Probit link handles models where tails are more "certain"
* Probit models also arise from a different modeling framework: a latent variable determined by covariates with random gaussian noise with a threshold for a "true" outcome


# Aside: Choice of exposure metric

* Observed vs **<red>model-predicted</red>**
* Which summary measure?
    * Cmin, Cmax, Cavg, AUC, \dots
    * Depends on the endpoint and substantive knowledge
* What time horizon? Depends on the context \dots
    * When dose 'holidays' or reductions are rare
        * "Early" (e.g., cycle 1)
        * Steady-state
    * When dose holidays or reductions are common
        * Average to the end of the study
        * Average up to the event (or end of study)
        * I tend to avoid cumulative measures due to confounding with time
        * Adds another layer of variability to exposures which should be modeled


# Workbook: Fitting a logistic regression model

# Model diagnostics and comparison

* We'll look at two main types of diagnostics for assessing model fit
    * Residual-based
    * Simulation-based

* Our main method for comparing models will be the quality of out-of-sample predictions


# Residual Diagnostics

* Common types of residuals: 
    * "Response" residuals (the usual `DV-PRED`). 
    * Deviance residuals.
    * Pearson residuals.
* For __all__ of these, lower your expectations: 
    * Plots usually look "chunky".
    * Natural consequence of binary data. 
    * Some sort of smoother needed to aid the eye. 
    
# Response residuals

* Easiest residuals to conceptualize. 
* Not expected to be "homoscedastic" or even symmetric around the $y=0$ line. 
* Still expected to be at the $y=0$ line "on average".
* Advantage: departures from $y=0$ are on the probability scale. 
* Don't be fooled: 
    * Consequential departures from the $y=0$ line can be obscured by the plotting scale.
    * Response residuals range from -1 to 1. 
    * Plot appearance can depend on distribution of covariates apart from the model 
    
# Example of response residuals

<!-- TO-DO add plot of residuals vs CAVGss from mod01 with smoother overlaid -->

::::{.columns}

:::{.column}

```{r, 'response-resid', fig.show = 'hide'}
dat_plus <- dat_mod
dat_plus$res <- residuals(mod01_glm, type = "response")
dat_plus$pred <- fitted(mod01_glm)

ggplot(dat_plus, aes(x = CAVGSS, y = res)) +
  geom_point() +
  geom_smooth() + 
  labs(x='Steady-state Cavg', y='Response residuals')

```
:::

:::{.column}
```{r, ref.label='response-resid', echo=FALSE, out.width="95%"}

```

:::

::::



# Other residuals  (Pearson, Deviance)  

* Pearson residuals are like standardizing residuals in linear models 
* Deviance residuals are the contributions to the log-likelihood of each data point
* Benefits: 
    * More nearly symmetric and homoscedastic (if model is correct).
    * Deviance residuals should be asymptotically normally distributed
* Suggestions: 
    * Start with response residuals. You know what these mean. 
    * Compare with deviance residuals. If the deviance residual plots look better, take comfort. 
    * Don't spend too much time trying to get any residual plots to look "good". They won't. Use them to suggest model refinements and then move on. 
* The total residual deviance should be "close" to the residaul degrees of freedom

# Example of deviance residuals

<!-- TO-DO add plot of residuals vs CAVGss from mod01 with smoother overlaid -->

::::{.columns}

:::{.column}

```{r, 'deviance-resid', fig.show = 'hide'}
dat_plus <- dat_mod
dat_plus$res <- residuals(mod01_glm, type = "deviance")
dat_plus$pred <- fitted(mod01_glm)

ggplot(dat_plus, aes(x = CAVGSS, y = res)) +
  geom_point() +
  geom_smooth() + 
  labs(x='Steady-state Cavg', y='Deviance residuals')

```
:::

:::{.column}
```{r, ref.label='deviance-resid', echo=FALSE, out.width="95%"}

```

:::

::::


# Simulation-based Diagnostics

* Quantile Residuals
    * Use model to simulate data and calculate quantile of observed values against simulated data
    * Smooths out discrete residual values
    * DHARMa Package in R makes this easy (`simulateResiduals`)
    
* Visual Predictive Checks (VPCs)
  * Recipe
    * Simulate many replicates of the DV using the estimated model and observed predictors
    * Summarize each simulated replicate
    * Plot distribution(s) of summary statistics
    * Overlay observed values
  * The choice of summary statistics is problem dependent
  * A place to start is bucketing covariates and assessing overall trends
  * Will see more examples in the next hands-on portion


# Quantile residuals

::::{.columns}

:::{.column width=47.5%}

```{r, 'dharma-resid', fig.show = 'hide'}
dharma_resids = simulateResiduals(mod01_glm,
                                  n = 1000,
                                  integerResponse = TRUE)

plot(dharma_resids)
```

```{r, ref.label='dharma-resid', echo=FALSE, out.width="95%"}

```


:::

:::{.column width=5%}
:::

:::{.column width=47.5%}

```{r, 'quantile-resid', fig.show = 'hide'}
aedat %>% ungroup() %>% 
  mutate(quantile_residual = dharma_resids$scaledResiduals) %>% 
  ggplot(aes(x=BWT, y=quantile_residual)) +
  geom_point() +
  geom_smooth() + 
  labs(y='Quantile residuals', x='Body Weight (kg)')
```

```{r, ref.label='quantile-resid', echo=FALSE, out.width="95%"}

```


:::
::::

# Simulate data for a VPC

```{r}
# Simulate using the simulate function in stats
aedat_pp = bind_cols(aedat,
                     stats::simulate(mod01_glm, nsim=500)) %>% 
  pivot_longer(cols=sim_1:sim_500)

aedat_pp %>% ungroup() %>% 
  select(USUBJID, PTTYPE, AE01, Quartile, name, value) %>% 
  slice_tail(n=4)
```


# Generate VPC for categorical predictor

::::{.columns}

:::{.column}

```{r, 'bayes-vpc', fig.show = 'hide'}
# Observed data summary
obs_summary <- aedat %>% 
  group_by(Quartile) %>% 
  summarise(phat_obs = mean(AE01))

#Simulated data summary
sim_summary <- aedat_pp %>% 
  group_by(name,Quartile) %>% 
  summarise(phat_sim = mean(value))

# VPC
sim_summary %>% 
  ggplot(aes(x=Quartile, y=phat_sim)) +
  geom_violin() +
  geom_point(data=obs_summary, aes(y=phat_obs), col='red',shape=3) +
  labs(x='', y='Proportion with AE') +
  coord_flip()
```
:::

:::{.column}
```{r, ref.label='bayes-vpc', echo=FALSE}

```

:::

::::



# VPC for continuous variable: define summary statistic

* Fit generalized additive model (smoother) to each simulated dataset
* Predict at a fixed grid of values (0 to 95$^{th}$ percentile)

```{r}
summary_function <- function(.data, .x_name, .y_name='value') {
  .data <- .data %>% ungroup() %>% rename('xvar' = .x_name, 'yvar'=.y_name)
  x_grid <- with(.data, seq(from = min(xvar),
                           to = quantile(xvar,probs = 0.95), 
                           length = 100))
  fit <- gam(yvar ~ s(xvar), family=binomial(link='logit'), data=.data)
  predictions <- predict(fit, newdata = data.frame(xvar=x_grid), type='response')
  return( data.frame(xvar=x_grid, prediction = predictions))
}
```

# Compute summary statistics for observed data

```{r}
obs_summary <- summary_function(aedat, .x_name = 'CAVGSS', .y_name='AE01') %>% 
  mutate(type='Observed')

head(obs_summary)
```

# Compute summary on each simulated study

```{r}
sim_summary <- aedat_pp %>% 
  # Nest everying except the simulation name
  nest(cols=-name) %>% 
  # Use 200 sims for demonstration
  slice(1:200) %>% 
  # Compute summary stats for each simulated dataset
  mutate(predictions = map(cols, ~summary_function(.x, .x_name='CAVGSS'))) %>% 
  select(name,predictions) %>% 
  unnest(cols=predictions) %>% 
  # Summarise across simulated data sets
  group_by(xvar) %>% 
  summarise(qlo = quantile(prediction, probs = 0.05),
            qhi = quantile(prediction, probs = 0.95),
            prediction=median(prediction)
            ) %>% 
  mutate(type = 'Simulated')
```

# Plot VPC

```{r, 'cont-vpc', fig.show='hide'}
sim_summary %>% bind_rows(obs_summary) %>% 
  ggplot(aes(x=xvar, y=prediction)) +
  geom_line(aes(col=type, group=type)) +
  geom_ribbon(aes(ymin=qlo, ymax=qhi, fill=type), alpha=0.2) +
  labs(x='Steady-state Cavg', y='Probability of severe AE')

```

```{r, ref.label='cont-vpc', echo=FALSE, out.width="90%"}

```



# Model Comparison

* Likelihood and information criteria
    * Likelihood (or -2 log-likelihood) measures <red>in-sample</red> model fit
    * Cross-validation to approximmate <red>out-of-sample</red> deviance
    * IC approximate <red>out-of-sample</red> deviance
        * AIC = -2*Log-likelihood + 2*k
        * BIC = -2*Log-likelihood + log(N)*k
        * Lower is better


* Classification accuracy
  * Classification accuracy scores (e.g. sensitivity, specificity, Kappa)
  * Receiver operating characteristic curve (ROC) and its AUC

# Workbook: Model evaluation and comparison

# What we haven't covered

* Interactions
    * Does the exposure-response relationship depend on anothe covariate?
    * Aka, effect modification

* Non-linear effects
    * Parametric (e.g., Emax) models for binary data not well served by R
    * Semi-parametric models (e.g., generalized additive models) are using the `mgcv` package
    
* Forest plots for covariate effects
    
* Clinical trial simulation

# Key learning objectives

At the end of today's session, I hope you will know:

* How to make effective exploratory analysis plots for binary data
    * We've seen plots for categorical and continuous predictors
    
* The basic concepts of maximum likelihood estimation for binary data models

* How to fit and evaluate binary data models
    * Using `glm` to  fit models
    * Using residuals and VPCs to evaluate models

* How to interpret model terms
    * Parameters tell us about odds ratios



# Break

\large

Next up: Bayesain analysis of binary data


# References
