---
title: "Exposure-response modeling for time-to-event data"
output:
  slidy_presentation:
#    theme: cerulean
    fig_width: 9
    fig_height: 5
    font_adjustment: 3
    transition: none
    css: slidystyles.css
    footer: Metrum Research Group &copy 2021
    mathjax: local
    self_contained: false
    df_print: kable
    variant: markdown+fancy_lists
bibliography: Day2-references.bib
csl: statistical-science.csl
#nocite: | 
editor_options: 
  chunk_output_type: console
---


```{r,echo=FALSE,message=FALSE}
knitr::opts_chunk$set(comment='.',fig.align=TRUE,message=FALSE,warning=FALSE)
library(tidyverse)
library(stringr)
library(haven)
library(survival)
library(survminer)
library(survMisc)
library(texreg)
library(flexsurv)

set.seed(314159)
theme_set(theme_bw())
``` 



```{r, echo=FALSE}
load('../data/aedat.RDS')

aedat <-
  aedat %>% 
  mutate(AETOXGR = factor(AETOXGR, 0:3, labels=c("None","Mild","Moderate","Severe")),
         ae_any = AETOXGR != 'None') %>% 
  group_by(USUBJID) %>%
  # End of study for patients without a severe event
  mutate(TTE_SEVERE = case_when(
    STUDYID=="PROTA" ~ 2,
    STUDYID=="PROTB" ~ 6
  ),
  # Time of severe event for those that had one
  TTE_SEVERE = ifelse(AETOXGR=="Severe", TTE, TTE_SEVERE),
  AE_any = ifelse(AETOXGR!="None", 1, 0)
  )

# Both for EDA and for model-checking, it's generally helpful to have quartiles of exposure:
dat_use <-
  aedat %>% arrange(USUBJID, TTE_SEVERE) %>% slice(1) %>%
  group_by(PBO) %>%
  mutate(Quartile = ifelse(PBO == "PBO", "PBO",
                           paste0("Q", ntile(CAVGSS, n = 4))))
```




# What is time-to-event data?
  
  - In clinical studies, we often measure the time to a specific event:
    - time to death
    - time to disease worsening
    - time to incident adverse event
    - time to abnormal lab value (e.g., AST > 3xULN)
    - time to infection
    - time to study discontinuation
    - duration of hospital visit

<!-- # Three essential components -->

<!--  - Well-defined event -->
<!--  - Clear time origin -->
<!--  - Defined time scale -->

<!-- :::: {style="padding: 0.5em; -->
<!--   background: LightGray; -->
<!--   color: black; -->
<!--   border: 2px black; -->
<!--   border-radius: 5px;"} -->
<!-- ::: {.center} -->
<!-- **Quiz** -->
<!-- ::: -->

<!-- What might be an event definition and time origin for _time to disease worsening_ in a clinical trial? -->
<!-- :::: -->


# Three essential components

 - Well-defined event
 - Clear time origin
 - Defined time scale
 
:::{.notebox}

::::{.center}
**Quiz**
::::

What might be an event definition and time origin for _time to disease worsening_ in a clinical trial?
:::


# What makes TTE data different?

- For some subjects, we may not observe an event
   - The time to event is **censored**

```{r, message=FALSE, echo=FALSE}
dtte = cgd %>% group_by(id,random,status) %>% 
  summarise(min_time = min(tstop)) %>% 
  group_by(id,random) %>% 
  arrange(min_time) %>% 
  slice(1) %>% 
  ungroup() %>% 
  mutate(start = random-min(random),
         end = start + min_time,
         status = factor(status,levels=0:1, labels=c('Censored','Event')),
         study_start = min(random))

p1 = dtte %>% 
  ggplot(aes(y=id)) +
  geom_point(aes(x=start)) +
  geom_point(aes(x=end, shape=status, fill=status,col=status)) +
  geom_segment(aes(x=start, xend=end, yend=id, col=status), alpha=0.3) +
  labs(x='Study time')

p2 = dtte %>% 
  ggplot(aes(y=id)) +
  geom_point(aes(x=0)) +
  geom_point(aes(x=end-start, shape=status, fill=status,col=status)) +
  geom_segment(aes(x=0, xend=end-start, yend=id, col=status), alpha=0.3) +
  labs(x='Subject time')

```

:::{.columns}

::::{.column}
```{r, out.width='95%', out.height='50%', echo=FALSE}
p1
```
::::

::::{.column}
```{r, out.width='95%', echo=FALSE}
p2
```
::::


:::
  
# A little notation

* There are two time-to-event processes happening:

   * $T$ = time to event of interest
   * $C$ = time to censoring

* With right censoring, we observe 
    * $T^* = \text{min}(T,C)$
    * $\delta = I(T \leq C)$

* We are trying to estimate the distribution of T, but we observe T*

  * We'll return to this when discussing model diagnostics

* Typical to assume that $T$ and $C$ are independent
  


# Types of censoring

- Right censoring
  - We know the event did not happen prior to time $b$ (i.e., we know $T > b$)
- Left censoring
  - We know the event happened before time $a$ (i.e., we know $T < a$)
- Interval censoring
  - We know the event happened between times $a$ and $b$ but not the exact time (i.e, $a < T < b$) 

- In clinical trials, we most often deal with right and interval censoring


# How does censoring introduce complexity?

- If we observed event times for all subjects, we could use 'standard' methods

- Hard to estimate probability density function when we don't see all events happening
  - A type of missing data problem

- Working with the <red>hazard function</red> alleviates some of the problems

- Hazard function = instantaneous event rate, conditional on event happening on or after time $t$
  - $h(t) = \lim_{\Delta t \rightarrow 0} \frac{P(t < T \leq t+\Delta t ~|~  T \geq t )}{\Delta t}$
  - "conditional on event happening on or after time $t$" is what helps

# Terminology

- Cumulative hazard = total hazard accumulated to time $t$
  - $H(t) = \int_0^t h(s) ds$
  - This is the expected number of events to time $t$ [@noauthor_2011-gh]
<br>
- Probability density function = instantaneous event risk (aka *density*)
  - $f(t) = \lim_{\Delta t \rightarrow 0} \frac{P(t < T \leq t+\Delta t)}{\Delta t} $
<br>
- Survival function = probability of an event happening after time $t$
  - $S(t) = P(T > t)$

# Connections

Two important relationships to remember:

* The relation between the survival function and the cumulative hazard
$$
\begin{align*}
S(t) &= \exp\left\{-H(t)\right\}
\end{align*}
$$

* We can derive the density function from the hazard and survival functions
$$
\begin{align*}
f(t) &= h(t) S(t)
\end{align*}
$$

<!-- - importance of survival function -->
- When estimating S(t), we (almost) always assume censoring is independent of event time

# Workbook 01: Relationship between functions

Let's look at the relationship between the hazard, cumulative hazard, density, and survival functions for some parametric distributions.
  
  
#  Non-parametric estimation of survival, cumulative hazard and hazard functions

* We'll start with non-parametric estimates of $S(t)$ and $H(t)$
* The most commonly used estimator of $S(t)$ is the <red>Kaplan-Meier</red> estimator
  - aka the ***product limit*** estimate

* The most common estimator of $H(t)$ is the `Nelson-Aalen` estimator
  - Can also estimate $S(t)$ as $\widehat{S_{FH}(t)} = \exp\left\{ \widehat{H_{NA}(t)} \right\}$
  - This is known as the Fleming-Harrington estimate of $S(t)$
  - Similar, but not identical, to K-M estimate

# Basics of Kaplan-Meier estimate

Suppose we have these 10 event times (in days): 
```{r, echo=FALSE}
lung %>% 
  mutate(status=status-1) %>% 
  arrange(time,status) %>%
  filter(time > 80) %>% 
  select(time,status) %>% 
  slice(1:10) %>% 
  mutate(event_time = if_else(status==0, paste0(time,'+'), as.character(time))) %>% 
  pull(event_time)
```
where a "+" denotes a censored observation.

How would you estimate 

>- $P(T > 80)$ ?   
>    - $P(T>80) = 1$ because all event times are after 80 days


>- $P(T > 90)$?  
>    - $P(T > 90) = 6/10$ because we know exactly 4 events happened before 90 days

>- $P(T > 94)$?
>    - $P(T > 94) = ?$  We know 3 events happened after 94 days, but what about the censored time at 92 days?


# Kaplan-Meier and conditional probability

It turns outs that we can use some basic probability calculations to estimate $S(t)$ in the presence of censoring.  

1. Divide time into distinct intervals (at each event time, $\tau_j$)    
2. For each interval $j$,    
    - Calculate the proportion of subjects with an event ($d_j$), among the subjects in the risk set for that interval ($r_j$)  
    - The <grn>risk set</grn> at time $t$ = N - # events prior to $t$ - # censored prior to $t$  
    - Calculate the probability of an event after the $j^{th}$ interval, conditional on no event prior to the interval as $1 - \frac{d_j}{r_j}$

3. Estimate $S(t)$ as the product of the conditional probabilities up to time $t$
    - $\hat{S}_{KM}(t) = \prod_{\tau_j \leq t} 1 - \frac{d_j}{r_j}$

# Kaplan-Meier estimation in R

Fortunately, we don't have to do that work by hand :)

The `survfit` function in the R package `survival` does the work for us:

:::{.center}
```{r}
fit0 <- survfit(Surv(TTE,AE_any) ~ 1, data=dat_use)
```
:::

- The `Surv(time, event)` function creates a survival response object
    - `time` = event or censoring time
    - `event` = event indicator (1=event, 0 = right censored)
    - More complex types of censoring can be handled

- RHS of formula cannot include continuous variables (*Why?*)
    - This is okay: `survfit(Surv(TTE, AE_any) ~ Quartile, data=dat_use)`
    - This is not: `survfit(Surv(TTE, AE_any) ~ CAVGSS, data=dat_use)`

# Basic Survfit output

The `survfit` object gives us some basic information:

```{r}
print(fit0)
```

# More Survfit output

We can get more detail and predicted values with `summmary`

```{r}
summary(fit0, times = seq(0,1,by=0.25))
```

We'll explore more in the hands-on section.


# Plotting the estimated survival function

The `survminer::ggsurvplot` function provides clean plots

```{r}
survminer::ggsurvplot(fit0, risk.table = TRUE)
```


# Workbook 02: Kaplan-Meier estimates and plots


# Summary measures of S(t)

* Median time to event (black dashed line)
* Event rate at time $t$ (blue dashed line)
* Restricted mean survival time (RMST) to $t^*$
    * (Unrestricted) mean survival may not be well-defined
    * RMST is the average event-free time up to $t^*$
    * Equivalent to the area under S(t) from 0 to $t^*$
    
```{r, echo=FALSE}
p1 <- survminer::ggsurvplot(fit0, conf.int = FALSE)
med <- quantile(fit0, probs=0.50)[['quantile']]
yr1 <- summary(fit0, time=1)$surv

p2 <- p1$plot +
  geom_segment(aes(x=0, xend=med,y=0.5, yend=0.5), linetype='dashed') +
  geom_segment(aes(x=med, xend=med,y=0.5, yend=0), 
               arrow=arrow(length = unit(0.03, "npc")),
               linetype='dashed') +
  geom_segment(aes(x=1, xend=1,y=0, yend=yr1), linetype='solid', color='blue') +
  geom_segment(aes(x=1, xend=0,y=yr1, yend=yr1), 
               arrow=arrow(length = unit(0.03, "npc")),
                     linetype='solid', color='blue') +
  geom_area(mapping = aes(x = ifelse(time>=0 & time<=3 , time, 0)),
          fill = "red", alpha = 0.1) 

p2
```

  
# Summary measures of S(t) in R: median
  - Median time to event
```{r}
quantile(fit0, probs=0.50) %>% unlist()
```

# Summary measures of S(t) in R: percentile

  - Percent surviving to times t=c(0,1,2)  

```{r}
summary(fit0, time=c(0,1,2))
```

# Summary measures of S(t) in R: RMST
  
  - Restricted mean survival time  
```{r}
print(fit0, rmean = 2)
```
  
```{r}
print(fit0, rmean = 4)
```


# Comparing two survival curves

- Hazard ratio
    - Most commonly used measure of effects
    - Closely tied to the Cox model
- Difference or ratio of median survival
    - Simple measure, easily understood
    - Connection to accelerated failure time models
- Difference or ratio of RMST
    - Re-emerging with treatments providing long-term cure fractions

:::{.notebox}

Which one(s) you use depends on which aspects of the survival distribution are important

:::

# Are these survival distributions the same?

```{r, echo=FALSE, out.width='90%'}

time_vector = seq(0,4,length=100)

hr_data <- tibble(a1=c(2,2,2),a2=c(2, 0.75,0.75),
       med1 = c(2,2,2), med2=c(3,2,3),
       comp=factor(paste("Scenario", c(1,2,3)))) %>% 
  mutate(b1=med1/log(2)^(1/a1),
         b2=med2/log(2)^(1/a2),
         surv1 = map2(a1,b1,~pweibull(time_vector, .x, .y, lower.tail = FALSE)),
         surv2 = map2(a2,b2,~pweibull(time_vector, .x, .y, lower.tail = FALSE))) %>% unnest(cols=c(surv1,surv2)) %>% 
  mutate(time=rep(time_vector,times=3),
         haz1 = (a1/b1)*(time/b1)^a1,
         haz2 = (a2/b2)*(time/b2)^a2,
         hr = haz1/haz2) 

hr_summary <- hr_data %>% 
  group_by(comp) %>% 
  summarise(hr = mean(hr,na.rm = TRUE), 
            med1=mean(med1), 
            med2=mean(med2),
            rmst1_2 = sum(surv1[time <=2])*diff(time_vector)[1],
            rmst2_2 = sum(surv2[time <=2])*diff(time_vector)[1],
            rmst1_4 = sum(surv1)*diff(time_vector)[1],
            rmst2_4 = sum(surv2)*diff(time_vector)[1]
) %>% 
  mutate(label_hr=paste0("HR: ",pmtables::sig(hr)),
         label_med=paste0("Med. diff: ",pmtables::sig(med1-med2)),
         label_rmst=paste0("RMST diff: ",pmtables::sig(rmst1_4-rmst2_4)))

hr_data %>% 
  ggplot(aes(x=time, y=surv1,color='Treatment')) + geom_line() + 
  geom_line(aes(y=surv2,color='Reference')) +
  facet_wrap(~comp)+
  ylim(0,1) +
  geom_text(data=hr_summary, x=1, y=0.25, aes(label=label_hr), col='black') +
  geom_text(data=hr_summary, x=1.4, y=0.15, aes(label=label_med), col='black') +
  geom_text(data=hr_summary, x=1.4, y=0.05, aes(label=label_rmst), col='black') +
  scale_color_manual(name='Group',
                     values=c("Treatment"="blue", "Reference"="red")) +
  labs(x='Time', y='Surviving fraction')



```



# Testing for differences in survival functions

$H_0: S_0(t) = S_1(t)$ vs $H_1: S_0(t) \ne S_1(t)$

- Log rank test
   - Equivalent to Mantel_Haenszel test for binary data, where stratification is at unique event times (see back-ups for more details)
   - Most powerful test when the hazard ratio is constant (but applicable even if it is not)
   - Gives relatively higher weight to later differences in S(t)

- Generalized Wilcoxan test
   - A weighted version of the log-rank test with weights proportional to the number at risk
   - Gives relatively higher weight to early differences in S(t)
   - Thus, more sensitive when differences in survival occur early


# Workbook 03: Summary measures from S(t) and comparing survival functions


# Semi-parametric models for TTE data

* Cox Proportional hazards model (the Cox model)
    - Baseline hazard is not estimated directly
* Connection to Poisson model
    - Breslow's formulation
* Piecewise exponential as a simplification
* Smooth baseline hazards
    - Provides a similar (though not identical) model with the ability to simulate



# Cox PH model

Originally proposed by Cox in 1972 @Cox1972-ul

:::{.notebox}
$$
h(t) = h_0(t) \exp(\theta_1 x_1 + \dots \theta_p x_p)
$$
Note: there is no intercept in the exponential ... <red>why?</red>
:::

$h_0(t)$ is referred to as the <red>baseline hazard</red>

* $h_0(t) > 0$ 
* The baseline hazard function is not specified
    * We'll see that it does not need to be specified in order to estimate $\theta$
* The covariate effects modify the hazard <red>proportionately</red>
    - The covariate model is linear in the parameters
    - Similar  to the linear predictor we saw in logistic regression


# Connection to survival function

Recall that 
$$
S(t) = \exp \left\{ -\int_0^t h(s) ~ ds \right\}
$$

Then, under the Cox model we have

$$
\begin{align*}
S(t) &= \exp \left\{ -\int_0^t h_0(s) \exp(\theta_1 x_1) ~ ds \right\} \\
 &= \exp \left\{ -\exp(\theta_1 x_1) ~ H_0(t) \right\} \\
 &= \left\{ S_0(t)\right\}^{\exp(\theta_1 x_1)}
\end{align*}
$$
where $S_0(t) = \exp\left\{ -H_0(t) \right\}$ is the <red>baseline survival function</red>


# Example of proportional hazards

```{r, echo=FALSE}

.tmp = data.frame(times = seq(0,4,length=100)) %>% 
  mutate(h0 = exp(-3 + 0.5 * sin(pi*times) + 0.5*times ),
         h1 = h0 * exp(0.33)) %>% 
  rowwise() %>% 
  mutate(H0 = map_dbl(times, ~integrate(f=function(t) {exp(-3 + 0.5 * sin(pi*t) + 0.5*t )},
                                        lower = 0, upper=.)$value),
         S0 = exp(-H0),
         S1 = exp(-H0*exp(0.33))
         )

p1 = .tmp %>% 
  ggplot(aes(x=times)) +
  geom_line(aes(y=h0)) +
  geom_line(aes(y=h1), color='red') +
  labs(x='Time', y='Hazard')

p2 = .tmp %>% 
  ggplot(aes(x=times)) +
  geom_line(aes(y=S0)) +
  geom_line(aes(y=S1), color='red') +
  labs(x='Time', y='Survival')

```

:::{.columns}

::::{.column}
```{r, echo=FALSE, out.width="90%"}
p1
```
::::

::::{.column}
```{r, echo=FALSE, out.width="90%"}
p2
```
::::

:::

# Parameter estimation

Cox proposed using a partial likelihood approach, treating the baseline hazard as an infinite dimensional nuisance parameter.

This yields the likelihood function (assuming unique event times):

$$
\ell(\theta | x) =  \prod_{i=1}^k \frac{\exp(x_i^T \theta)}{\sum_{j \in R(i)} \exp(x_j^T \theta)}
$$
where 

* there are $k$ unique event times
* $R(j)$ is the set of subjects at risk for an event at $t(j)$

Note that the product is only over the $k$ events, not over all subjects.


# Parameter interpretation

Suppose we have the model
$$
h(t) = h_0(t) \exp(\theta_1 x_1 + \theta_2 x_2)
$$
where $x_1$ is binary and $x_2$ is continuous.

* Then, $e^{\theta_1}$ represents the hazard ratio comparing $x_1=1$ to $x_1=0$
    - $\theta_1$ represents the log hazard ratio

* $e^{\theta_2}$ represents the hazard ratio for a one unit difference in $x_2$
    - $\exp(\theta_2 \times d)$ represents the hazard ratio for a $d$ unit difference in $x_2$


# Example model

As an example, let's fit the model:
$$
h(t) = h_0(t) \exp\left\{ \theta_1 \cdot Q_1 + \theta_2 \cdot Q_2 + \theta_3 \cdot Q_3 + \theta_4 \cdot Q_4 \right\}
$$
where the covariates $Q_1, \dots, Q_4$ are indicators for exposure quartile

$$
Q_j = \begin{cases} 1 & \text{CAVGSS in quartile } j \\
  0 & \text{otherwise}\end{cases}
$$

Questions:

* What does $h_0(t)$ correspond to?
* What does $\theta_1$ correspond to?

# Estimation of the Cox model in R

To fit the Cox model in R, we'll use the `coxph` function:

```{r}
fit1 <- coxph(Surv(TTE, AE_any) ~ Quartile, data=dat_use)
```

* Similar LHS as used with `survfit` (for K-M estimate)
* RHS can include factor or continuous variables
  - Character variables are converted to factors


# Output from `coxph`

```{r, echo=TRUE}
print(fit1)
```

```{r, echo=FALSE}
ci = pmtables::sig(exp(confint(fit1))["QuartileQ1",])

hr1 = pmtables::sig(exp(coef(fit1))[1])

est = glue::glue("{hr1} ({paste(ci,collapse=',')})")
```

* Hazard ratio comparing Q1 to placebo: `r est`

# Estimation of the baseline cumulative hazard function from a Cox model

* Recall, estimation of covariate effects did not need an estimate of $h_0(t)$

* However, we can obtain a post-hoc estimate of $H_0(t)$ and $S_0(t)$

$$
\widehat{H_0}(t) = \sum_{j: t(j) \leq t} \frac{D_j}{\sum_{i \in R(j)} \exp(\hat{\theta}_1 x_{1i} + \dots \hat{\theta}_p x_{pi})}
$$
where 

* $t(j)$ are the ordered unique event times
* $R(j)$ is the set of subjects at risk for an event at $t(j)$
* $D_j$ is the number of events at time $t(j)

# Baseline hazard estimation: intuition

$$
\widehat{H_0}(t) = \sum_{j: t(j) \leq t} \frac{D_j}{\sum_{i \in R(j)} \exp(\hat{\theta}_1 x_{1i} + \dots \hat{\theta}_p x_{pi})}
$$

* With no covariates, this simplifies to the Nelson-Aalen estimator
* With covariates, the denominator is the counter-factual number of subjects at risk had all subjects been in the reference group

The baseline survival function is $\widehat{S_0}(t) = \exp\left\{ -\widehat{H_0}(t) \right\}$

The subject-specific survival function is
$$
\widehat{S_i}(t) = \left[ \widehat{S_0}(t) \right] ^{\exp(\hat{\theta}_1 x_{1i} + \dots + \hat{\theta}_p x_{pi})} 
$$

# Model evaluation for the Cox model

Our primary model evaluation tools will be:

* Comparing model predicted and observed survival
* Martingale and deviance residuals for functional form of covariate effects
* Assessing the PH assumption
* Concordance / Harrell's C-index


# Comparing model predicted and observed survival (1)

To extract predicted survival curves from a Cox model, we'll use the `survfit` function

```{r}
# One line per unique covariate pattern
dat_pred <- dat_use %>% 
  arrange(Quartile) %>% 
  distinct(Quartile)

# Use survfit to extract predicted survival function
# survfit0 adds time=0 to the predictions; helps with plotting
cox_preds_fit1 <- survfit(fit1, newdata = dat_pred) %>% 
  survfit0() 

head(cbind(cox_preds_fit1$time, cox_preds_fit1$surv))

```

# Comparing model predicted and observed survival (2)

:::{.columns}

::::{.column}

```{r, "coxpred", fig.show='hide'}
survival_preds_fit1 <- cox_preds_fit1$surv %>% 
  as.data.frame() %>% 
  mutate(time = cox_preds_fit1$time) %>% 
  pivot_longer(cols=-time) %>% 
  mutate(group = as.numeric(name),
         Quartile = paste0(dat_pred$Quartile[group]))

ggsurvplot(survfit(Surv(TTE, AE_any) ~ Quartile, data=dat_use), 
           data=dat_use)$plot +
  geom_step(data=survival_preds_fit1,
            aes(x=time, y=value), linetype='dashed') +
  facet_wrap(~Quartile
             )

```

::::

::::{.column}
```{r, ref.label="coxpred", echo=FALSE, out.width="100%"}

```

::::

:::


# Martingale residuals

The Martingale residual is defined as

$$
M_i = \delta_i - \widehat{H_i}(T^*_i)
$$
where 

$$
\delta_i = \cases{1 & event \\
0 & censored}
$$
and
$T^*$ is the observed event or censoring time

:::{.notebox}

Because $H(t)$ is is expected number of events according to the model, the Martingale residual is an "observed - expected" type of residual.

:::

Often asymmetric due to the fact that $M_i \in (-\infty, 1)$

# Deviance residuals

The deviance residual is defined as

$$
D_i = \text{sign}(M_i) \sqrt{-2 \left( M_i + \delta_i \log \widehat{H_i}(T^*_i)\right) }
$$

* More symmetric than Martingale residuals
* Roughly have mean=0 and sd=1

# Uses for Martingale and Deviance residuals

:::{.columns}

::::{.column}

* Plot vs linear predictor to assess exponential link
* Plot vs covariate to assess functional form
    - If model is correct, should see no trends

```{r,"devplot", fig.show='hide'}
dat_use <- dat_use %>% ungroup() %>% 
  mutate(dev_resids = residuals(fit1, type = "deviance"))

ggplot(dat_use, aes(x=BWT, y=dev_resids)) +
  geom_point() +
  geom_point() + geom_smooth() +
  labs(x='Body weight (kg)', y='Deviance residual')

```
::::

::::{.column}
```{r, ref.label='devplot', echo=FALSE, out.width="95%"}

```

::::

:::


# Assessing PH assumption

* Plot standardized Schoenfeld residuals vs time
    - Shows how coefficient changes with time
    - Flat line if PH model is correct
* Test for significance of including time-by-covariate interactions in the model
* Compare predicted and observed survival

# Assessing PH assumption: example

:::{.columns}

::::{.column}

```{r}
assess_ph_fit1 <- cox.zph(fit1)

print(assess_ph_fit1)

```

* This is the score test for adding a time-by-covariate interaction.
  - Indicates a violation of the PH assumption
  - Also suggested by the plot of the standardized Schoenfeld residuals vs time
  - The observed and predicted S(t) plots did not'
  
::::

::::{.column}

```{r, out.width="95%"}
plot(assess_ph_fit1)
```

::::

:::

# Concordance as a measure of model fit

Concordance is a measure of how well the predicted risk of an event aligns with the observed risk.

Imagine you have two randomly selected subjects $i$ and $j$ with 

* Covariates $x_i$ and $x_j$
* Risk scores: $\eta_i = x_i^T \hat{\theta}$ and $\eta_j = x_j^T \hat{\theta}$
* (True) event times: $T_i$ and $T_j$

Then concordance (c-index) estimates: $P(T_i > T_j ~|~ \eta_i < \eta_j)$

Harrell et al. @Harrell1982-ux proposed a method for handling censored data.

A model with higher c-index should provide better predictions.


# Workbook 04: Cox regression

   - Fitting the Cox model
   - Model evaluation
   - Comparing models
 


# Cox model as a Poisson regression

It turns out that we can get the same likelihood if we view the data as arising from a Poisson process with intervals defined by the unique event times. @Whitehead1980-fl

* Divide time into non-overlapping intervals with segments defined by observed event times
* Constant baseline hazard within each interval

:::{.notebox}

This yields another way of thinking about the Cox model as a data generating model.

* Data are arising from a continuous baseline hazard which we estimate with piecewise constants
* Piecewise constant hazard = piecewise exponential model

:::


# Piecewise exponential as an approximation to the Cox model

* This means that one approximation to a Cox model is
    - Piecewise exponential with predefined intervals
    - The more intervals, the closer the result is to Cox model

The model is given by 
$$h(t) = h_0(t) \exp(\theta_1 x_1 + \dots + \theta_p x_p)$$ 
where

$$
h_0(t) = \lambda_j \text{ for } t \in [\tau_{j-1} ,  \tau_j)
$$

* How to define the time intervals depends on 
    - Clinical knowledge (e.g., transplant @French2012-qv )
    - Time-varying predictors
    - Expected number and timing of events

* This formulation provides a semi-parametric model from which we can simulate survival data


# Smooth non-parametric baseline hazard functions 

* An alternative semi-parametric model is to estimate the baseline hazard using smoothing splines @Royston2002-vc

* Can be fitted using the `flexsurv` R package and in `brms`

* Royston and Parmar  @Royston2002-vc propose $g(S(t)) = s(t) + \beta^T x$

    - $s(t)$ is a smooth function modeled using cubic smoothing splines
    - $g(S) = \log( -\log(S))$ corresponds to a proportional hazards model
    - $g(S) = \log(1/S - 1)$ corresponds to a proportional odds model

* `brms` implementation uses M-splines for the baseline hazard

    - M-splines are non-negative and integrate to I-splines
    - Simplifies calculations


# Extensions to standard Cox model

* Stratified Cox model 
    - Allows different  baseline hazard by strata
    - $h(t) = \color{red}{h_{0,s}(t)} \exp(\theta_1 x_1 + \dots + \theta_p x_p)$
    - Often used in primary analysis of TTE endpoints in clinical trials (e.g., stratify by region)
* Time-varying covariates 
xs    - Allows for covariates to be constant over intervals defined by the data (similar to how NONMEM treats time varying covariates)
    - Doesn't allow for continuously varying covariates
    - Frequently used when conditioning on an intermediate event (e.g., Stanford heart transplant data)
    


#  Introduction to Parametric models

* The Cox model was designed to estimate effects of covariates on the time-to-event distribution.
    - With that focus, the baseline hazard isn't of much interest.
* However, in PMx modeling we are often interested in modeling the full time-to-event distribution
    * Both the baseline hazard and covariate effects

* This will be our focus for the rest of the course
    * Accelerated failure time models in R and Stan
    * General hazard models in Stan

# Traditional parametric models

Many of the commonly used models fall under the accelerated failure time framework (Wei, @Wei1992-lp)

$$
\log (T) = \mu + \theta x + \epsilon
$$

:::{.notebox}

| ***Distribution for $\epsilon$*** |  ***Distribution for T*** |
|---------------------------------- |---------------------------|
| Extreme value              | Weibull |
| Extreme value (scale=1)    | Exponential |
| Normal                     | Log-normal |
| Logistic                   | Log-logistic |

:::


But any distribution for $T$ that has support on values of $T \ge 0$ is suitable, including Gompertz, Gamma, Generalized Gamma, etc.

Nice, conceptual introductions to (parametric) TTE modeling are by Holford @Holford2013-mh  and Bradburn et al. @Bradburn2003-fg


# AFT vs PH models

* Covariate effects in AFT models are fundamentally different than in PH models
* In a PH model, a covariate scales the hazard function
    - $h(t) = h_0(t) \exp(\theta x)$
    - $S(t) = S_0(t)^{\exp(\theta x)}$
* In an AFT model, a covariate scales time
    - $\log T = \mu + \theta x + \epsilon$
    - $S(t) = S_0(ct)$, where $c = \exp(\theta x)$ is the **acceleration factor**
    - The covariate effect is on the percentiles of the distribution
    - $\frac{\text{Percentile when } x=1}{\text{Percentile when } x=0} = \frac{1}{c}$ for all percentiles

# Features of common distributions

* Exponential
  - Hazard is constant
  - One parameter
* Weibull, Gompertz, and Gamma
  - Two parameters (scale/location and shape)
  - Hazard in monotonically increasing or decreasing
* Log-logistic and log-normal
  - Two parameters (scale/location and shape)
  - Hazard is uni-modal
  - Falling, or arc (rising then falling)
* Generalized Gamma
  - Three parameters (scale/location and shape)
  - Monotonic (increasing or decreasing); arc; bathtub


# Likelihood for parametric survival models

Assuming that censoring times are independent of event times, then the individual contribution to the likelihood function is 

$$
L_i(\theta) = \begin{cases}
f(T^*_i) & \text{for } \delta_i = 1 \\
S(T^*_i) & \text{for } \delta_i = 0
\end{cases}
$$
and the likelihood function is

$$
\begin{align*}
L(\theta) &= \prod_{i=1}^n f(T^*_i)^{\delta_i} S(T^*_i)^{1-\delta_i} \\
          & = \prod_{i=1}^n h(T^*_i)^{\delta_i} S(T^*_i)
\end{align*}
$$

# Fitting parametric TTE models in R

* The `survreg()` function in the `survival` package
* The `flexsurvreg()` function in the `flexsurv` package

We generally use the `flexsurv` package because there are more distributions available

```{r}
fit_01 <- flexsurvreg(Surv(TTE_SEVERE, AE01) ~ CAVGSS, 
                      data = dat_use,
                      dist = 'weibull')
```

# Model evaluation

* Residual plots
  - Similar use as with the Cox model
  
* Simulation-based diagnostics
  - VPCs for survival and hazard functions
  - NPDEs

# Numeric model comparison

* AIC and friends
* C-index

# Workbook 5: Parametric models


# References
